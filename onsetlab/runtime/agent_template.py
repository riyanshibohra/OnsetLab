#!/usr/bin/env python3
"""
{agent_name} - AI Agent with MCP + API Tool Execution
=====================================================
Auto-generated by OnsetLab

Usage:
    python agent.py --interactive
    python agent.py "Show me open issues in facebook/react"

Requirements:
    - Python 3.8+
    - Node.js + npx (for MCP servers)
    - llama-cpp-python
    - requests (for API tools)
"""

import argparse
import asyncio
import json
import os
import re
import sys
from typing import Optional

# {api_tools_import}

# ============================================================
# CONFIGURATION
# ============================================================

AGENT_NAME = "{agent_name}"
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
HAS_API_TOOLS = {has_api_tools}

def load_config():
    """Load configuration files."""
    # System prompt
    prompt_path = os.path.join(SCRIPT_DIR, "system_prompt.txt")
    with open(prompt_path, "r") as f:
        system_prompt = f.read()
    
    # MCP config
    config_path = os.path.join(SCRIPT_DIR, "mcp_config.json")
    with open(config_path, "r") as f:
        mcp_config = json.load(f)
    
    return system_prompt, mcp_config

SYSTEM_PROMPT, MCP_CONFIG = load_config()
ALLOWED_TOOLS = set(MCP_CONFIG.get("allowed_tools", []))


# ============================================================
# MCP CLIENT - Communicates with MCP servers via JSON-RPC
# ============================================================

class MCPClient:
    """Async client for communicating with an MCP server."""
    
    def __init__(self, name: str, command: str, args: list, env: dict = None):
        self.name = name
        self.command = command
        self.args = args
        self.env = {**os.environ, **(env or {})}
        self.process: Optional[asyncio.subprocess.Process] = None
        self.request_id = 0
        self.initialized = False
    
    async def start(self) -> bool:
        """Start the MCP server subprocess."""
        try:
            # Resolve environment variables
            resolved_env = {}
            for key, value in self.env.items():
                if isinstance(value, str) and value.startswith("${"):
                    var_name = value[2:-1]
                    resolved_env[key] = os.getenv(var_name, "")
                else:
                    resolved_env[key] = value
            
            self.process = await asyncio.create_subprocess_exec(
                self.command,
                *self.args,
                stdin=asyncio.subprocess.PIPE,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env={**os.environ, **resolved_env},
            )
            
            # Initialize MCP connection
            init_result = await self._send_request("initialize", {
                "protocolVersion": "2024-11-05",
                "capabilities": {},
                "clientInfo": {"name": "onsetlab-agent", "version": "1.0.0"}
            })
            
            if init_result and not init_result.get("error"):
                await self._send_notification("notifications/initialized", {})
                self.initialized = True
                return True
            else:
                print(f"   ‚ö†Ô∏è  Failed to initialize {self.name}: {init_result}")
                return False
                
        except FileNotFoundError:
            print(f"   ‚ùå Command not found: {self.command}")
            print(f"      Make sure Node.js and npx are installed")
            return False
        except Exception as e:
            print(f"   ‚ùå Failed to start {self.name}: {e}")
            return False
    
    async def call_tool(self, tool_name: str, arguments: dict, timeout: float = 30.0) -> str:
        """Call a tool on the MCP server with timeout."""
        if not self.initialized:
            return f"Error: MCP server {self.name} not initialized"
        
        try:
            result = await asyncio.wait_for(
                self._send_request("tools/call", {
                    "name": tool_name,
                    "arguments": arguments
                }),
                timeout=timeout
            )
            
            if result.get("error"):
                return f"Error: {result['error'].get('message', 'Unknown error')}"
            
            # Extract text content from result
            content = result.get("result", {}).get("content", [])
            if content and isinstance(content, list):
                texts = [c.get("text", "") for c in content if c.get("type") == "text"]
                return "\n".join(texts) if texts else str(result)
            return str(result)
            
        except asyncio.TimeoutError:
            return f"Error: Tool call timed out after {timeout}s"
        except Exception as e:
            return f"Error calling tool: {e}"
    
    async def _send_request(self, method: str, params: dict) -> dict:
        """Send JSON-RPC request and get response."""
        self.request_id += 1
        request = {
            "jsonrpc": "2.0",
            "id": self.request_id,
            "method": method,
            "params": params
        }
        
        request_line = json.dumps(request) + "\n"
        self.process.stdin.write(request_line.encode())
        await self.process.stdin.drain()
        
        # Read response
        response_line = await self.process.stdout.readline()
        if response_line:
            return json.loads(response_line.decode())
        return {"error": {"message": "No response from server"}}
    
    async def _send_notification(self, method: str, params: dict):
        """Send JSON-RPC notification (no response expected)."""
        notification = {
            "jsonrpc": "2.0",
            "method": method,
            "params": params
        }
        notification_line = json.dumps(notification) + "\n"
        self.process.stdin.write(notification_line.encode())
        await self.process.stdin.drain()
    
    async def stop(self):
        """Stop the MCP server."""
        if self.process:
            try:
                self.process.terminate()
                await asyncio.wait_for(self.process.wait(), timeout=5.0)
            except:
                self.process.kill()


# ============================================================
# TOOL MANAGER - Routes to MCP servers or API functions
# ============================================================

class ToolManager:
    """
    Manages both MCP servers and direct API tools.
    
    Routes tool calls to:
    - MCP servers (via JSON-RPC) for services with MCP support
    - API functions (via api_tools.py) for direct REST API services
    """
    
    def __init__(self, config: dict):
        self.config = config
        self.mcp_clients: dict[str, MCPClient] = {}
        self.tool_to_mcp: dict[str, str] = {}  # tool -> MCP server name
        self.api_tools = set(config.get("api_tools", []))
        self.mcp_tools = set(config.get("mcp_tools", []))
        self.allowed_tools = set(config.get("allowed_tools", []))
    
    async def start_all(self):
        """Start all configured servers."""
        # Start MCP servers
        mcp_servers = self.config.get("mcpServers", {})
        if mcp_servers:
            print("\nüîå Starting MCP servers...")
            for name, server_cfg in mcp_servers.items():
                client = MCPClient(
                    name=name,
                    command=server_cfg.get("command", "npx"),
                    args=server_cfg.get("args", []),
                    env=server_cfg.get("env", {})
                )
                
                success = await client.start()
                if success:
                    self.mcp_clients[name] = client
                    for tool in server_cfg.get("tools", []):
                        self.tool_to_mcp[tool] = name
                    print(f"   ‚úÖ {name} started")
                else:
                    print(f"   ‚ùå {name} failed to start")
        
        # Report API tools (no startup needed)
        if self.api_tools:
            print(f"\nüîó API tools ready: {len(self.api_tools)} tools")
        
        if not self.mcp_clients and not self.api_tools:
            print("\n‚ö†Ô∏è  No tool backends available. Tool calls will fail.")
    
    async def call_tool(self, tool_name: str, params: dict) -> str:
        """
        Route tool call to correct backend (MCP or API).
        
        Order of routing:
        1. Check if tool is allowed (whitelist)
        2. Check if tool is an API tool -> call via api_tools.py
        3. Check if tool is an MCP tool -> call via MCP server
        4. Fallback: try MCP servers
        """
        
        # Validate tool is allowed
        if self.allowed_tools and tool_name not in self.allowed_tools:
            available = ", ".join(sorted(self.allowed_tools)[:5])
            return f"Tool '{tool_name}' is not available. Try: {available}..."
        
        # Check if this is an API tool
        if tool_name in self.api_tools:
            return self._call_api_tool(tool_name, params)
        
        # Route to MCP
        return await self._call_mcp_tool(tool_name, params)
    
    def _call_api_tool(self, tool_name: str, params: dict) -> str:
        """Call an API tool (sync, runs in thread)."""
        if HAS_API_TOOLS:
            try:
                result = call_api_tool(tool_name, params)
                return json.dumps(result, indent=2) if isinstance(result, dict) else str(result)
            except Exception as e:
                return f"Error calling API tool: {e}"
        else:
            return f"API tools not available for: {tool_name}"
    
    async def _call_mcp_tool(self, tool_name: str, params: dict) -> str:
        """Call an MCP tool."""
        # Find server for this tool
        server_name = self.tool_to_mcp.get(tool_name)
        if not server_name:
            # Try any available server
            for name in self.mcp_clients:
                server_name = name
                break
        
        if not server_name or server_name not in self.mcp_clients:
            return f"No server available for tool: {tool_name}"
        
        client = self.mcp_clients[server_name]
        return await client.call_tool(tool_name, params)
    
    async def stop_all(self):
        """Stop all MCP servers gracefully."""
        if self.mcp_clients:
            print("\nüîå Stopping MCP servers...")
            for name, client in self.mcp_clients.items():
                await client.stop()
                print(f"   ‚úÖ {name} stopped")


# Backward compatibility alias
MCPManager = ToolManager


# ============================================================
# MODEL - Load and run the fine-tuned model
# ============================================================

def load_model():
    """Load the GGUF model using llama-cpp-python."""
    try:
        from llama_cpp import Llama
    except ImportError:
        print("‚ùå llama-cpp-python not installed.")
        print("   Run: pip install llama-cpp-python")
        sys.exit(1)
    
    # Find GGUF file
    gguf_file = None
    for f in os.listdir(SCRIPT_DIR):
        if f.endswith(".gguf"):
            gguf_file = os.path.join(SCRIPT_DIR, f)
            break
    
    if not gguf_file:
        print("‚ùå No .gguf file found!")
        print(f"   Looked in: {SCRIPT_DIR}")
        sys.exit(1)
    
    print(f"\nüß† Loading {os.path.basename(gguf_file)}...")
    model = Llama(
        model_path=gguf_file,
        n_ctx=4096,
        n_gpu_layers=-1,
        verbose=False,
    )
    print("   ‚úÖ Model loaded")
    return model


def generate_response(model, messages: list) -> str:
    """Generate a response using the model."""
    # Format for Qwen chat template
    prompt = ""
    for msg in messages:
        role = msg["role"]
        content = msg["content"]
        if role == "system":
            prompt += f"<|im_start|>system\n{content}<|im_end|>\n"
        elif role == "user":
            prompt += f"<|im_start|>user\n{content}<|im_end|>\n"
        elif role == "assistant":
            prompt += f"<|im_start|>assistant\n{content}<|im_end|>\n"
        elif role == "tool":
            prompt += f"<|im_start|>tool\n{content}<|im_end|>\n"
    
    prompt += "<|im_start|>assistant\n"
    
    output = model(prompt, max_tokens=512, temperature=0.3, stop=["<|im_end|>", "</tool_call>"])
    response = output["choices"][0]["text"]
    
    if "<tool_call>" in response and "</tool_call>" not in response:
        response += "</tool_call>"
    
    return response.strip()


def parse_tool_call(response: str) -> dict:
    """Parse tool call from response."""
    if "<tool_call>" not in response:
        return None
    
    # Extract JSON from <tool_call> tags
    start = response.find("<tool_call>") + len("<tool_call>")
    end = response.find("</tool_call>")
    if end == -1:
        end = len(response)
    
    json_str = response[start:end].strip()
    
    # Find JSON object
    brace_start = json_str.find("{")
    brace_end = json_str.rfind("}") + 1
    
    if brace_start >= 0 and brace_end > brace_start:
        try:
            return json.loads(json_str[brace_start:brace_end])
        except json.JSONDecodeError:
            pass
    return None


# ============================================================
# AGENT LOOP - Main execution loop
# ============================================================

async def run_agent(tools: ToolManager, model, query: str) -> str:
    """Run the agentic loop with MCP + API tool execution."""
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": query},
    ]
    
    max_iterations = 5
    for _ in range(max_iterations):
        response = generate_response(model, messages)
        tool_call = parse_tool_call(response)
        
        if tool_call:
            tool_name = tool_call.get("tool")
            params = tool_call.get("parameters", {})
            
            print(f"\nüîß Calling {tool_name}...")
            result = await tools.call_tool(tool_name, params)
            print(f"üìã Result: {result[:200]}...")
            
            messages.append({"role": "assistant", "content": response})
            messages.append({"role": "tool", "content": result})
        else:
            return response
    
    return "Max iterations reached."


async def interactive_mode(tools: ToolManager, model):
    """Interactive chat mode."""
    print(f"\nü§ñ {AGENT_NAME} Ready!")
    print(f"   Tools: {len(ALLOWED_TOOLS)} available")
    
    # Show tool breakdown
    mcp_count = len(tools.mcp_tools)
    api_count = len(tools.api_tools)
    if mcp_count and api_count:
        print(f"   Backend: {mcp_count} MCP + {api_count} API tools")
    elif mcp_count:
        print(f"   Backend: {mcp_count} MCP tools")
    elif api_count:
        print(f"   Backend: {api_count} API tools")
    
    print("   Type 'quit' to exit")
    print("-" * 50)
    
    while True:
        try:
            query = input("\nYou: ").strip()
            if query.lower() in ("quit", "exit", "q"):
                print("Goodbye!")
                break
            if not query:
                continue
            
            response = await run_agent(tools, model, query)
            print(f"\nü§ñ Agent: {response}")
                
        except KeyboardInterrupt:
            print("\nGoodbye!")
            break
        except Exception as e:
            print(f"\n‚ùå Error: {e}")


async def async_main(query: str = None, interactive: bool = False):
    """Async main function."""
    # Load environment
    try:
        from dotenv import load_dotenv
        load_dotenv()
    except ImportError:
        pass
    
    # Check for MCP requirements (only if we have MCP servers)
    if MCP_CONFIG.get("mcpServers"):
        import shutil
        if not shutil.which("npx"):
            print("‚ö†Ô∏è  npx not found. MCP servers require Node.js.")
            print("   Install from: https://nodejs.org/")
    
    # Load model
    model = load_model()
    
    # Start tool backends (MCP + API)
    tools = ToolManager(MCP_CONFIG)
    await tools.start_all()
    
    try:
        if interactive or not query:
            await interactive_mode(tools, model)
        else:
            response = await run_agent(tools, model, query)
            print(response)
    finally:
        await tools.stop_all()


def main():
    parser = argparse.ArgumentParser(description=f"{AGENT_NAME} - AI Agent")
    parser.add_argument("query", nargs="?", help="Query to process")
    parser.add_argument("--interactive", "-i", action="store_true", help="Interactive mode")
    args = parser.parse_args()
    
    asyncio.run(async_main(query=args.query, interactive=args.interactive))


if __name__ == "__main__":
    main()
