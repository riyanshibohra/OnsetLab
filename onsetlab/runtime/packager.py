"""
Agent Packager
==============
Generates runnable agent packages with all necessary files.

Supports two runtime options:
1. Ollama (GGUF) - Simple CLI: `ollama run my-agent`
2. Python - Direct script: `python agent.py`
"""

import json
import os
from dataclasses import dataclass
from enum import Enum
from typing import Optional


class RuntimeType(Enum):
    """Supported runtime types."""
    OLLAMA = "ollama"      # GGUF model with Ollama
    PYTHON = "python"      # Python script with transformers
    BOTH = "both"          # Generate both options


@dataclass
class PackageConfig:
    """Configuration for agent packaging."""
    runtime: RuntimeType = RuntimeType.BOTH
    agent_name: str = "my_agent"
    output_dir: str = "./agent_package"
    include_training_data: bool = False  # Include training data in package
    include_readme: bool = True


class AgentPackager:
    """
    Packages a trained agent for deployment.
    
    Generates:
    - For Ollama: Modelfile, config.yaml
    - For Python: agent.py, requirements.txt
    - Common: README.md, system_prompt.txt, mcp_config.json
    """
    
    def __init__(
        self,
        agent_name: str,
        system_prompt: str,
        tools: list,
        mcp_servers: list,
        model_path: str = None,
        config: PackageConfig = None,
    ):
        self.agent_name = agent_name
        self.system_prompt = system_prompt
        self.tools = tools
        self.mcp_servers = mcp_servers
        self.model_path = model_path
        self.config = config or PackageConfig(agent_name=agent_name)
    
    def package(self) -> str:
        """
        Create the agent package.
        
        Returns path to the package directory.
        """
        output_dir = self.config.output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"ðŸ“¦ Packaging agent to: {output_dir}")
        
        # Common files
        self._write_system_prompt(output_dir)
        self._write_mcp_config(output_dir)
        
        # Runtime-specific files
        if self.config.runtime in (RuntimeType.OLLAMA, RuntimeType.BOTH):
            self._write_ollama_files(output_dir)
        
        if self.config.runtime in (RuntimeType.PYTHON, RuntimeType.BOTH):
            self._write_python_files(output_dir)
        
        # README
        if self.config.include_readme:
            self._write_readme(output_dir)
        
        print(f"âœ… Agent packaged successfully!")
        return output_dir
    
    def _write_system_prompt(self, output_dir: str):
        """Write system prompt file."""
        path = os.path.join(output_dir, "system_prompt.txt")
        with open(path, "w") as f:
            f.write(self.system_prompt)
        print(f"   ðŸ“„ system_prompt.txt")
    
    def _write_mcp_config(self, output_dir: str):
        """Write MCP server configuration."""
        config = {
            "mcpServers": {}
        }
        
        for server in self.mcp_servers:
            server_dict = server.to_dict() if hasattr(server, 'to_dict') else server
            name = server_dict.get("package", "").split("/")[-1].replace("-mcp", "")
            config["mcpServers"][name] = {
                "command": "npx",
                "args": ["-y", server_dict.get("package", "")],
                "env": {
                    server_dict.get("env_var", "API_KEY"): f"${{{server_dict.get('env_var', 'API_KEY')}}}"
                }
            }
        
        path = os.path.join(output_dir, "mcp_config.json")
        with open(path, "w") as f:
            json.dump(config, f, indent=2)
        print(f"   ðŸ“„ mcp_config.json")
    
    def _write_ollama_files(self, output_dir: str):
        """Write Ollama-specific files."""
        # Modelfile
        gguf_path = self.model_path or "./model.gguf"
        modelfile = f'''# Ollama Modelfile for {self.agent_name}
# 
# Usage:
#   ollama create {self.agent_name} -f Modelfile
#   ollama run {self.agent_name}

FROM {gguf_path}

SYSTEM """{self.system_prompt}"""

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER stop "<|im_end|>"
PARAMETER stop "</tool_call>"
'''
        
        path = os.path.join(output_dir, "Modelfile")
        with open(path, "w") as f:
            f.write(modelfile)
        print(f"   ðŸ“„ Modelfile (Ollama)")
    
    def _write_python_files(self, output_dir: str):
        """Write Python runtime files."""
        # agent.py
        tools_json = json.dumps(
            [t.to_dict() if hasattr(t, 'to_dict') else t for t in self.tools],
            indent=2
        )
        
        agent_py = f'''#!/usr/bin/env python3
"""
{self.agent_name} - AI Agent
============================
Auto-generated by OnsetLab

Usage:
    python agent.py "What's on my calendar today?"
    python agent.py --interactive
"""

import argparse
import json
import os
import sys

# Configuration
AGENT_NAME = "{self.agent_name}"
MODEL_PATH = "{self.model_path or './model'}"
SYSTEM_PROMPT = """{self.system_prompt}"""

TOOLS = {tools_json}


def load_model():
    """Load the fine-tuned model."""
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
    except ImportError:
        print("Error: transformers not installed.")
        print("Run: pip install -r requirements.txt")
        sys.exit(1)
    
    print(f"Loading model from {{MODEL_PATH}}...")
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None,
    )
    
    return model, tokenizer


def generate_response(model, tokenizer, query: str) -> str:
    """Generate a response for the given query."""
    messages = [
        {{"role": "system", "content": SYSTEM_PROMPT}},
        {{"role": "user", "content": query}},
    ]
    
    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
    )
    
    if hasattr(model, "device"):
        inputs = inputs.to(model.device)
    
    outputs = model.generate(
        input_ids=inputs,
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Extract assistant response
    if "assistant" in response.lower():
        parts = response.split("assistant")
        response = parts[-1].strip()
    
    return response


def parse_tool_call(response: str) -> dict:
    """Parse tool call from response if present."""
    if "<tool_call>" in response:
        start = response.find("<tool_call>") + len("<tool_call>")
        end = response.find("</tool_call>")
        if end > start:
            try:
                return json.loads(response[start:end])
            except json.JSONDecodeError:
                pass
    return None


def interactive_mode(model, tokenizer):
    """Run in interactive mode."""
    print(f"\\nðŸ¤– {{AGENT_NAME}} Ready!")
    print("Type your message (or 'quit' to exit)")
    print("-" * 40)
    
    while True:
        try:
            query = input("\\nYou: ").strip()
            if query.lower() in ("quit", "exit", "q"):
                print("Goodbye!")
                break
            if not query:
                continue
            
            response = generate_response(model, tokenizer, query)
            tool_call = parse_tool_call(response)
            
            if tool_call:
                print(f"\\nðŸ”§ Tool Call: {{tool_call['name']}}")
                print(f"   Args: {{json.dumps(tool_call.get('arguments', {{}}), indent=2)}}")
            else:
                print(f"\\nAgent: {{response}}")
                
        except KeyboardInterrupt:
            print("\\nGoodbye!")
            break


def main():
    parser = argparse.ArgumentParser(description=f"{{AGENT_NAME}} - AI Agent")
    parser.add_argument("query", nargs="?", help="Query to process")
    parser.add_argument("--interactive", "-i", action="store_true", help="Interactive mode")
    args = parser.parse_args()
    
    model, tokenizer = load_model()
    
    if args.interactive or not args.query:
        interactive_mode(model, tokenizer)
    else:
        response = generate_response(model, tokenizer, args.query)
        tool_call = parse_tool_call(response)
        
        if tool_call:
            print(json.dumps(tool_call, indent=2))
        else:
            print(response)


if __name__ == "__main__":
    main()
'''
        
        path = os.path.join(output_dir, "agent.py")
        with open(path, "w") as f:
            f.write(agent_py)
        os.chmod(path, 0o755)  # Make executable
        print(f"   ðŸ“„ agent.py (Python)")
        
        # requirements.txt
        requirements = '''# Requirements for running the agent
torch>=2.0.0
transformers>=4.36.0
accelerate>=0.25.0
'''
        
        path = os.path.join(output_dir, "requirements.txt")
        with open(path, "w") as f:
            f.write(requirements)
        print(f"   ðŸ“„ requirements.txt")
    
    def _write_readme(self, output_dir: str):
        """Write README with usage instructions."""
        runtime = self.config.runtime
        
        readme = f'''# {self.agent_name}

AI Agent built with [OnsetLab](https://github.com/your-repo/onsetlab)

## Quick Start

'''
        
        if runtime in (RuntimeType.OLLAMA, RuntimeType.BOTH):
            readme += '''### Option 1: Ollama (Recommended)

```bash
# Create the model in Ollama
ollama create {name} -f Modelfile

# Run the agent
ollama run {name}
```

'''.format(name=self.agent_name)
        
        if runtime in (RuntimeType.PYTHON, RuntimeType.BOTH):
            readme += '''### Option 2: Python

```bash
# Install dependencies
pip install -r requirements.txt

# Run interactively
python agent.py --interactive

# Or with a query
python agent.py "What's on my calendar today?"
```

'''
        
        readme += f'''## Available Tools

'''
        for tool in self.tools:
            tool_dict = tool.to_dict() if hasattr(tool, 'to_dict') else tool
            readme += f"- **{tool_dict.get('name', 'unknown')}**: {tool_dict.get('description', '')}\n"
        
        readme += f'''
## MCP Servers

This agent uses the following MCP servers:

'''
        for server in self.mcp_servers:
            server_dict = server.to_dict() if hasattr(server, 'to_dict') else server
            readme += f"- `{server_dict.get('package', 'unknown')}` ({server_dict.get('auth_type', 'unknown')} auth)\n"
        
        readme += '''
## Configuration

- `system_prompt.txt` - The agent's system prompt
- `mcp_config.json` - MCP server configuration

## License

Built with OnsetLab
'''
        
        path = os.path.join(output_dir, "README.md")
        with open(path, "w") as f:
            f.write(readme)
        print(f"   ðŸ“„ README.md")
