"""
Docker Export - Generate Dockerfile and supporting files for containerized agent.

Creates a complete Docker setup that bundles:
- OnsetLab SDK
- Agent configuration
- Entrypoint script
- Optional: Ollama (if include_ollama=True)
- Optional: vLLM (if engine="vllm") — 5-10x faster inference
"""

import os
from pathlib import Path
from typing import Optional

from .config_export import ConfigExporter


# Dockerfile template - lightweight, uses external Ollama
DOCKERFILE_TEMPLATE = '''# OnsetLab Agent Container
# Generated by OnsetLab SDK

FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy agent configuration and entrypoint
COPY agent_config.yaml .
COPY entrypoint.py .

# Expose port for API mode
EXPOSE 8000

# Environment variables
ENV OLLAMA_HOST=http://host.docker.internal:11434
ENV ONSETLAB_CONFIG=/app/agent_config.yaml

# Run agent
ENTRYPOINT ["python", "entrypoint.py"]
CMD ["--serve", "--port", "8000"]
'''

# Dockerfile with bundled Ollama
DOCKERFILE_WITH_OLLAMA_TEMPLATE = '''# OnsetLab Agent Container (with Ollama)
# Generated by OnsetLab SDK

FROM ollama/ollama:latest as ollama

FROM python:3.11-slim

WORKDIR /app

# Copy Ollama binary
COPY --from=ollama /bin/ollama /usr/local/bin/ollama

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy agent configuration and entrypoint
COPY agent_config.yaml .
COPY entrypoint.py .
COPY start.sh .
RUN chmod +x start.sh

# Expose ports
EXPOSE 8000 11434

# Environment variables
ENV OLLAMA_HOST=http://localhost:11434
ENV ONSETLAB_CONFIG=/app/agent_config.yaml
ENV OLLAMA_MODEL={model}

# Start script runs Ollama + Agent
ENTRYPOINT ["./start.sh"]
'''

# ---------------------------------------------------------------------------
# vLLM templates — GPU-accelerated inference (5-10x faster than Transformers)
# ---------------------------------------------------------------------------

DOCKERFILE_VLLM_TEMPLATE = '''# OnsetLab Agent Container (vLLM Engine)
# Generated by OnsetLab SDK
#
# vLLM provides 5-10x faster inference via:
# - PagedAttention (efficient GPU memory)
# - Continuous batching (concurrent requests)
# - KV cache optimization
#
# Requires: NVIDIA GPU with CUDA support

FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04

WORKDIR /app

# System deps
RUN apt-get update && apt-get install -y \\
    python3 python3-pip python3-dev \\
    && rm -rf /var/lib/apt/lists/*

# Install vLLM + OnsetLab
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# Copy agent configuration and entrypoint
COPY agent_config.yaml .
COPY entrypoint_vllm.py .

# vLLM OpenAI-compatible API port + OnsetLab agent port
EXPOSE 8000 8100

# Environment
ENV MODEL_NAME={model}
ENV VLLM_PORT=8100
ENV ONSETLAB_CONFIG=/app/agent_config.yaml
ENV VLLM_HOST=http://localhost:8100/v1

COPY start_vllm.sh .
RUN chmod +x start_vllm.sh

ENTRYPOINT ["./start_vllm.sh"]
'''

REQUIREMENTS_VLLM_TEMPLATE = '''onsetlab
pyyaml
requests
uvicorn
fastapi
vllm>=0.6.0
openai
'''

ENTRYPOINT_VLLM_TEMPLATE = '''#!/usr/bin/env python3
"""
OnsetLab Agent Entrypoint (vLLM engine)

Uses vLLM's OpenAI-compatible API for fast inference.
The vLLM server runs as a sidecar process on port 8100.
"""

import argparse
import os
import sys
import yaml
from openai import OpenAI

def load_config():
    config_path = os.environ.get("ONSETLAB_CONFIG", "agent_config.yaml")
    with open(config_path) as f:
        return yaml.safe_load(f)

class VLLMModel:
    """Use vLLM's OpenAI-compatible API as the model backend."""

    def __init__(self, model_name: str, base_url: str = "http://localhost:8100/v1"):
        self.model_name = model_name
        self.client = OpenAI(base_url=base_url, api_key="not-needed")

    def generate(self, prompt: str, max_tokens: int = 512, temperature: float = 0.0) -> str:
        resp = self.client.completions.create(
            model=self.model_name,
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        return resp.choices[0].text.strip()

def create_agent(config):
    from onsetlab import Agent
    from onsetlab.tools import Calculator, DateTime, UnitConverter, TextProcessor, RandomGenerator

    TOOL_MAP = {
        "Calculator": Calculator,
        "DateTime": DateTime,
        "UnitConverter": UnitConverter,
        "TextProcessor": TextProcessor,
        "RandomGenerator": RandomGenerator,
    }

    tools = []
    for tool_config in config.get("tools", []):
        tool_class = TOOL_MAP.get(tool_config["class"])
        if tool_class:
            tools.append(tool_class())

    settings = config.get("onsetlab", {}).get("settings", {})

    # Use vLLM model backend
    vllm_host = os.environ.get("VLLM_HOST", "http://localhost:8100/v1")
    model_name = os.environ.get("MODEL_NAME", config.get("onsetlab", {}).get("model", "Qwen/Qwen2.5-7B-Instruct"))
    model = VLLMModel(model_name, base_url=vllm_host)

    agent = Agent(
        model=model,
        tools=tools,
        memory=settings.get("memory", True),
        verify=settings.get("verify", True),
        routing=settings.get("routing", True),
        react_fallback=settings.get("react_fallback", True),
    )

    return agent

def serve(agent, host: str, port: int):
    from fastapi import FastAPI
    from pydantic import BaseModel
    import uvicorn

    app = FastAPI(title="OnsetLab Agent (vLLM)", version="1.0")

    class Query(BaseModel):
        message: str

    class Response(BaseModel):
        answer: str
        strategy: str
        slm_calls: int

    @app.post("/chat", response_model=Response)
    def chat(query: Query):
        result = agent.run(query.message)
        return Response(
            answer=result.answer,
            strategy=result.strategy_used,
            slm_calls=result.slm_calls,
        )

    @app.get("/health")
    def health():
        return {"status": "ok", "engine": "vllm"}

    uvicorn.run(app, host=host, port=port)

def interactive(agent):
    print(f"OnsetLab Agent (vLLM engine)")
    print("Type 'quit' to exit\\n")

    while True:
        try:
            query = input("You: ").strip()
            if query.lower() in ["quit", "exit", "q"]:
                break
            if not query:
                continue
            result = agent.run(query)
            print(f"Agent: {result.answer}\\n")
        except KeyboardInterrupt:
            break
    print("Goodbye!")

def main():
    parser = argparse.ArgumentParser(description="OnsetLab Agent (vLLM)")
    parser.add_argument("--serve", action="store_true", help="Run as API server")
    parser.add_argument("--host", default="0.0.0.0", help="Server host")
    parser.add_argument("--port", type=int, default=8000, help="Server port")
    args = parser.parse_args()

    config = load_config()
    agent = create_agent(config)

    if args.serve:
        serve(agent, args.host, args.port)
    else:
        interactive(agent)

if __name__ == "__main__":
    main()
'''

START_VLLM_SCRIPT_TEMPLATE = '''#!/bin/bash
# Start vLLM server + OnsetLab Agent

echo "Starting vLLM server for model: $MODEL_NAME ..."
python3 -m vllm.entrypoints.openai.api_server \\
    --model "$MODEL_NAME" \\
    --port "${VLLM_PORT:-8100}" \\
    --max-model-len 4096 \\
    --gpu-memory-utilization 0.9 &
VLLM_PID=$!

# Wait for vLLM to be ready
echo "Waiting for vLLM to initialize..."
for i in $(seq 1 60); do
    if curl -s http://localhost:${VLLM_PORT:-8100}/health > /dev/null 2>&1; then
        echo "vLLM is ready!"
        break
    fi
    sleep 2
done

# Start OnsetLab agent
python3 entrypoint_vllm.py "$@"

# Cleanup
kill $VLLM_PID 2>/dev/null
'''

# Requirements file
REQUIREMENTS_TEMPLATE = '''onsetlab
pyyaml
requests
uvicorn
fastapi
'''

# Entrypoint script
ENTRYPOINT_TEMPLATE = '''#!/usr/bin/env python3
"""
OnsetLab Agent Entrypoint
Generated by OnsetLab SDK
"""

import argparse
import os
import sys
import yaml

def load_config():
    """Load agent configuration."""
    config_path = os.environ.get("ONSETLAB_CONFIG", "agent_config.yaml")
    with open(config_path) as f:
        return yaml.safe_load(f)

def create_agent(config):
    """Create agent from configuration."""
    from onsetlab import Agent
    from onsetlab.tools import Calculator, DateTime, UnitConverter, TextProcessor, RandomGenerator
    
    # Map tool names to classes
    TOOL_MAP = {
        "Calculator": Calculator,
        "DateTime": DateTime,
        "UnitConverter": UnitConverter,
        "TextProcessor": TextProcessor,
        "RandomGenerator": RandomGenerator,
    }
    
    # Create tools
    tools = []
    for tool_config in config.get("tools", []):
        tool_class = TOOL_MAP.get(tool_config["class"])
        if tool_class:
            tools.append(tool_class())
    
    # Create agent
    settings = config.get("onsetlab", {}).get("settings", {})
    agent = Agent(
        model=config.get("onsetlab", {}).get("model", "phi3.5"),
        tools=tools,
        memory=settings.get("memory", True),
        verify=settings.get("verify", True),
        routing=settings.get("routing", True),
        react_fallback=settings.get("react_fallback", True),
    )
    
    return agent

def serve(agent, host: str, port: int):
    """Run agent as API server."""
    from fastapi import FastAPI
    from pydantic import BaseModel
    import uvicorn
    
    app = FastAPI(title="OnsetLab Agent", version="1.0")
    
    class Query(BaseModel):
        message: str
    
    class Response(BaseModel):
        answer: str
        strategy: str
        slm_calls: int
    
    @app.post("/chat", response_model=Response)
    def chat(query: Query):
        result = agent.run(query.message)
        return Response(
            answer=result.answer,
            strategy=result.strategy_used,
            slm_calls=result.slm_calls,
        )
    
    @app.get("/health")
    def health():
        return {"status": "ok", "model": agent.model_name}
    
    uvicorn.run(app, host=host, port=port)

def interactive(agent):
    """Run agent in interactive mode."""
    print(f"OnsetLab Agent ({agent.model_name})")
    print("Type 'quit' to exit")
    print()
    
    while True:
        try:
            query = input("You: ").strip()
            if query.lower() in ["quit", "exit", "q"]:
                break
            if not query:
                continue
            
            result = agent.run(query)
            print(f"Agent: {result.answer}")
            print()
        except KeyboardInterrupt:
            break
    
    print("Goodbye!")

def main():
    parser = argparse.ArgumentParser(description="OnsetLab Agent")
    parser.add_argument("--serve", action="store_true", help="Run as API server")
    parser.add_argument("--host", default="0.0.0.0", help="Server host")
    parser.add_argument("--port", type=int, default=8000, help="Server port")
    parser.add_argument("--interactive", action="store_true", help="Interactive mode")
    args = parser.parse_args()
    
    config = load_config()
    agent = create_agent(config)
    
    if args.serve:
        serve(agent, args.host, args.port)
    else:
        interactive(agent)

if __name__ == "__main__":
    main()
'''

# Start script for Ollama bundle
START_SCRIPT_TEMPLATE = '''#!/bin/bash
# Start Ollama and OnsetLab Agent

# Start Ollama in background
ollama serve &
OLLAMA_PID=$!

# Wait for Ollama to be ready
echo "Waiting for Ollama to start..."
sleep 5

# Pull model if not present
ollama pull $OLLAMA_MODEL || true

# Start agent
python entrypoint.py "$@"

# Cleanup
kill $OLLAMA_PID 2>/dev/null
'''


class DockerExporter:
    """Export agent as Docker container setup."""
    
    @classmethod
    def export(
        cls,
        agent,
        output: str,
        include_ollama: bool = False,
        engine: str = "ollama",
        api_mode: bool = True,
    ) -> str:
        """
        Export agent as Docker setup.
        
        Args:
            agent: Agent instance
            output: Output directory path
            include_ollama: Bundle Ollama in the container
            engine: Inference engine — "ollama" (default) or "vllm" (GPU, 5-10x faster)
            api_mode: Configure for API server mode
            
        Returns:
            Path to output directory
        """
        output_path = Path(output)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Export agent config
        config_path = output_path / "agent_config.yaml"
        ConfigExporter.export(agent, str(config_path))

        # --------------- vLLM engine ---------------
        if engine == "vllm":
            return cls._export_vllm(output_path, agent)

        # --------------- Ollama engine (default) ---------------
        
        # Generate Dockerfile
        dockerfile_path = output_path / "Dockerfile"
        if include_ollama:
            dockerfile_content = DOCKERFILE_WITH_OLLAMA_TEMPLATE.format(
                model=agent.model_name
            )
        else:
            dockerfile_content = DOCKERFILE_TEMPLATE
        
        with open(dockerfile_path, "w") as f:
            f.write(dockerfile_content)
        
        # Generate requirements.txt
        requirements_path = output_path / "requirements.txt"
        with open(requirements_path, "w") as f:
            f.write(REQUIREMENTS_TEMPLATE)
        
        # Generate entrypoint.py
        entrypoint_path = output_path / "entrypoint.py"
        with open(entrypoint_path, "w") as f:
            f.write(ENTRYPOINT_TEMPLATE)
        
        # Generate start.sh for Ollama bundle
        if include_ollama:
            start_path = output_path / "start.sh"
            with open(start_path, "w") as f:
                f.write(START_SCRIPT_TEMPLATE)
        
        # Generate docker-compose.yml for convenience
        compose_path = output_path / "docker-compose.yml"
        cls._generate_compose(compose_path, agent, include_ollama)
        
        # Generate README
        readme_path = output_path / "README.md"
        cls._generate_readme(readme_path, include_ollama)
        
        return str(output_path)

    @classmethod
    def _export_vllm(cls, output_path: Path, agent) -> str:
        """Export with vLLM engine — GPU-accelerated inference."""
        # Map Ollama model names to HuggingFace model IDs
        HF_MODEL_MAP = {
            "phi3.5": "microsoft/Phi-3.5-mini-instruct",
            "qwen2.5:7b": "Qwen/Qwen2.5-7B-Instruct",
            "qwen3-a3b": "Qwen/Qwen3-A3B",
            "mistral:7b": "mistralai/Mistral-7B-Instruct-v0.3",
            "gemma3:4b": "google/gemma-3-4b-it",
            "llama3.2:3b": "meta-llama/Llama-3.2-3B-Instruct",
        }
        hf_model = HF_MODEL_MAP.get(agent.model_name, agent.model_name)

        # Dockerfile
        with open(output_path / "Dockerfile", "w") as f:
            f.write(DOCKERFILE_VLLM_TEMPLATE.format(model=hf_model))

        # requirements.txt
        with open(output_path / "requirements.txt", "w") as f:
            f.write(REQUIREMENTS_VLLM_TEMPLATE)

        # Entrypoint
        with open(output_path / "entrypoint_vllm.py", "w") as f:
            f.write(ENTRYPOINT_VLLM_TEMPLATE)

        # Start script
        with open(output_path / "start_vllm.sh", "w") as f:
            f.write(START_VLLM_SCRIPT_TEMPLATE)

        # docker-compose.yml
        cls._generate_vllm_compose(output_path / "docker-compose.yml", hf_model)

        # README
        cls._generate_vllm_readme(output_path / "README.md", hf_model)

        return str(output_path)
    
    @classmethod
    def _generate_compose(cls, path: Path, agent, include_ollama: bool):
        """Generate docker-compose.yml."""
        if include_ollama:
            content = f'''version: '3.8'

services:
  agent:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_MODEL={agent.model_name}
'''
        else:
            content = f'''version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

  agent:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      - ollama

volumes:
  ollama_data:
'''
        
        with open(path, "w") as f:
            f.write(content)
    
    @classmethod
    def _generate_readme(cls, path: Path, include_ollama: bool):
        """Generate README with instructions."""
        if include_ollama:
            content = '''# OnsetLab Agent (Docker)

This directory contains everything needed to run your OnsetLab agent in Docker.

## Quick Start

```bash
# Build and run
docker-compose up --build

# Or build manually
docker build -t my-agent .
docker run -p 8000:8000 my-agent
```

## API Usage

```bash
# Health check
curl http://localhost:8000/health

# Chat
curl -X POST http://localhost:8000/chat \\
  -H "Content-Type: application/json" \\
  -d '{"message": "What is 15 + 27?"}'
```

## Interactive Mode

```bash
docker run -it my-agent --interactive
```
'''
        else:
            content = '''# OnsetLab Agent (Docker)

This directory contains everything needed to run your OnsetLab agent in Docker.

**Note:** This setup requires Ollama running separately. Use docker-compose for easy setup.

## Quick Start

```bash
# Start with docker-compose (includes Ollama)
docker-compose up --build

# Wait for Ollama to download the model (first run only)
```

## Manual Setup

If running without docker-compose:

1. Start Ollama: `ollama serve`
2. Pull model: `ollama pull phi3.5`
3. Build: `docker build -t my-agent .`
4. Run: `docker run -p 8000:8000 -e OLLAMA_HOST=http://host.docker.internal:11434 my-agent`

## API Usage

```bash
# Health check
curl http://localhost:8000/health

# Chat
curl -X POST http://localhost:8000/chat \\
  -H "Content-Type: application/json" \\
  -d '{"message": "What is 15 + 27?"}'
```
'''
        
        with open(path, "w") as f:
            f.write(content)

    # ------------------------------------------------------------------
    # vLLM helpers
    # ------------------------------------------------------------------

    @classmethod
    def _generate_vllm_compose(cls, path: Path, hf_model: str):
        """Generate docker-compose.yml for vLLM engine."""
        content = f'''version: '3.8'

services:
  agent:
    build: .
    ports:
      - "8000:8000"
    environment:
      - MODEL_NAME={hf_model}
      - VLLM_PORT=8100
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
'''
        with open(path, "w") as f:
            f.write(content)

    @classmethod
    def _generate_vllm_readme(cls, path: Path, hf_model: str):
        """Generate README for vLLM deployment."""
        content = f'''# OnsetLab Agent (vLLM Engine)

GPU-accelerated inference — 5-10x faster than standard Transformers.

**Model:** `{hf_model}`

## Requirements

- NVIDIA GPU with CUDA 12.1+
- Docker with NVIDIA Container Toolkit
- ~16 GB VRAM for 7B models, ~8 GB for 3-4B models

## Quick Start

```bash
# Build and run
docker-compose up --build

# Or manually
docker build -t my-agent-vllm .
docker run --gpus all -p 8000:8000 my-agent-vllm
```

## API Usage

```bash
curl http://localhost:8000/health

curl -X POST http://localhost:8000/chat \\
  -H "Content-Type: application/json" \\
  -d '{{"message": "What is 15 + 27?"}}'
```

## Performance

| Engine       | Throughput     | Latency | GPU Required |
|-------------|---------------|---------|-------------|
| Ollama       | Baseline       | ~2-5s   | Optional    |
| **vLLM**    | **5-10x**      | **<1s** | **Yes**     |

## Switching Models

Change the `MODEL_NAME` env var in `docker-compose.yml`:

```yaml
environment:
  - MODEL_NAME=Qwen/Qwen2.5-7B-Instruct  # or any HuggingFace model
```
'''
        with open(path, "w") as f:
            f.write(content)
