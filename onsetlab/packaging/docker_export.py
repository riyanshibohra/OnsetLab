"""
Docker Export - Generate Dockerfile and supporting files for containerized agent.

Creates a complete Docker setup that bundles:
- OnsetLab SDK
- Agent configuration
- Entrypoint script
- Optional: Ollama (if include_ollama=True)
"""

import os
from pathlib import Path
from typing import Optional

from .config_export import ConfigExporter


# Dockerfile template - lightweight, uses external Ollama
DOCKERFILE_TEMPLATE = '''# OnsetLab Agent Container
# Generated by OnsetLab SDK

FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy agent configuration and entrypoint
COPY agent_config.yaml .
COPY entrypoint.py .

# Expose port for API mode
EXPOSE 8000

# Environment variables
ENV OLLAMA_HOST=http://host.docker.internal:11434
ENV ONSETLAB_CONFIG=/app/agent_config.yaml

# Run agent
ENTRYPOINT ["python", "entrypoint.py"]
CMD ["--serve", "--port", "8000"]
'''

# Dockerfile with bundled Ollama
DOCKERFILE_WITH_OLLAMA_TEMPLATE = '''# OnsetLab Agent Container (with Ollama)
# Generated by OnsetLab SDK

FROM ollama/ollama:latest as ollama

FROM python:3.11-slim

WORKDIR /app

# Copy Ollama binary
COPY --from=ollama /bin/ollama /usr/local/bin/ollama

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy agent configuration and entrypoint
COPY agent_config.yaml .
COPY entrypoint.py .
COPY start.sh .
RUN chmod +x start.sh

# Expose ports
EXPOSE 8000 11434

# Environment variables
ENV OLLAMA_HOST=http://localhost:11434
ENV ONSETLAB_CONFIG=/app/agent_config.yaml
ENV OLLAMA_MODEL={model}

# Start script runs Ollama + Agent
ENTRYPOINT ["./start.sh"]
'''

# Requirements file
REQUIREMENTS_TEMPLATE = '''onsetlab
pyyaml
requests
uvicorn
fastapi
'''

# Entrypoint script
ENTRYPOINT_TEMPLATE = '''#!/usr/bin/env python3
"""
OnsetLab Agent Entrypoint
Generated by OnsetLab SDK
"""

import argparse
import os
import sys
import yaml

def load_config():
    """Load agent configuration."""
    config_path = os.environ.get("ONSETLAB_CONFIG", "agent_config.yaml")
    with open(config_path) as f:
        return yaml.safe_load(f)

def create_agent(config):
    """Create agent from configuration."""
    from onsetlab import Agent
    from onsetlab.tools import Calculator, DateTime, UnitConverter, TextProcessor, RandomGenerator
    
    # Map tool names to classes
    TOOL_MAP = {
        "Calculator": Calculator,
        "DateTime": DateTime,
        "UnitConverter": UnitConverter,
        "TextProcessor": TextProcessor,
        "RandomGenerator": RandomGenerator,
    }
    
    # Create tools
    tools = []
    for tool_config in config.get("tools", []):
        tool_class = TOOL_MAP.get(tool_config["class"])
        if tool_class:
            tools.append(tool_class())
    
    # Create agent
    settings = config.get("onsetlab", {}).get("settings", {})
    agent = Agent(
        model=config.get("onsetlab", {}).get("model", "phi3.5"),
        tools=tools,
        memory=settings.get("memory", True),
        verify=settings.get("verify", True),
        routing=settings.get("routing", True),
        react_fallback=settings.get("react_fallback", True),
    )
    
    return agent

def serve(agent, host: str, port: int):
    """Run agent as API server."""
    from fastapi import FastAPI
    from pydantic import BaseModel
    import uvicorn
    
    app = FastAPI(title="OnsetLab Agent", version="1.0")
    
    class Query(BaseModel):
        message: str
    
    class Response(BaseModel):
        answer: str
        strategy: str
        slm_calls: int
    
    @app.post("/chat", response_model=Response)
    def chat(query: Query):
        result = agent.run(query.message)
        return Response(
            answer=result.answer,
            strategy=result.strategy_used,
            slm_calls=result.slm_calls,
        )
    
    @app.get("/health")
    def health():
        return {"status": "ok", "model": agent.model_name}
    
    uvicorn.run(app, host=host, port=port)

def interactive(agent):
    """Run agent in interactive mode."""
    print(f"OnsetLab Agent ({agent.model_name})")
    print("Type 'quit' to exit")
    print()
    
    while True:
        try:
            query = input("You: ").strip()
            if query.lower() in ["quit", "exit", "q"]:
                break
            if not query:
                continue
            
            result = agent.run(query)
            print(f"Agent: {result.answer}")
            print()
        except KeyboardInterrupt:
            break
    
    print("Goodbye!")

def main():
    parser = argparse.ArgumentParser(description="OnsetLab Agent")
    parser.add_argument("--serve", action="store_true", help="Run as API server")
    parser.add_argument("--host", default="0.0.0.0", help="Server host")
    parser.add_argument("--port", type=int, default=8000, help="Server port")
    parser.add_argument("--interactive", action="store_true", help="Interactive mode")
    args = parser.parse_args()
    
    config = load_config()
    agent = create_agent(config)
    
    if args.serve:
        serve(agent, args.host, args.port)
    else:
        interactive(agent)

if __name__ == "__main__":
    main()
'''

# Start script for Ollama bundle
START_SCRIPT_TEMPLATE = '''#!/bin/bash
# Start Ollama and OnsetLab Agent

# Start Ollama in background
ollama serve &
OLLAMA_PID=$!

# Wait for Ollama to be ready
echo "Waiting for Ollama to start..."
sleep 5

# Pull model if not present
ollama pull $OLLAMA_MODEL || true

# Start agent
python entrypoint.py "$@"

# Cleanup
kill $OLLAMA_PID 2>/dev/null
'''


class DockerExporter:
    """Export agent as Docker container setup."""
    
    @classmethod
    def export(
        cls,
        agent,
        output: str,
        include_ollama: bool = False,
        api_mode: bool = True,
    ) -> str:
        """
        Export agent as Docker setup.
        
        Args:
            agent: Agent instance
            output: Output directory path
            include_ollama: Bundle Ollama in the container
            api_mode: Configure for API server mode
            
        Returns:
            Path to output directory
        """
        output_path = Path(output)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Export agent config
        config_path = output_path / "agent_config.yaml"
        ConfigExporter.export(agent, str(config_path))
        
        # Generate Dockerfile
        dockerfile_path = output_path / "Dockerfile"
        if include_ollama:
            dockerfile_content = DOCKERFILE_WITH_OLLAMA_TEMPLATE.format(
                model=agent.model_name
            )
        else:
            dockerfile_content = DOCKERFILE_TEMPLATE
        
        with open(dockerfile_path, "w") as f:
            f.write(dockerfile_content)
        
        # Generate requirements.txt
        requirements_path = output_path / "requirements.txt"
        with open(requirements_path, "w") as f:
            f.write(REQUIREMENTS_TEMPLATE)
        
        # Generate entrypoint.py
        entrypoint_path = output_path / "entrypoint.py"
        with open(entrypoint_path, "w") as f:
            f.write(ENTRYPOINT_TEMPLATE)
        
        # Generate start.sh for Ollama bundle
        if include_ollama:
            start_path = output_path / "start.sh"
            with open(start_path, "w") as f:
                f.write(START_SCRIPT_TEMPLATE)
        
        # Generate docker-compose.yml for convenience
        compose_path = output_path / "docker-compose.yml"
        cls._generate_compose(compose_path, agent, include_ollama)
        
        # Generate README
        readme_path = output_path / "README.md"
        cls._generate_readme(readme_path, include_ollama)
        
        return str(output_path)
    
    @classmethod
    def _generate_compose(cls, path: Path, agent, include_ollama: bool):
        """Generate docker-compose.yml."""
        if include_ollama:
            content = f'''version: '3.8'

services:
  agent:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_MODEL={agent.model_name}
'''
        else:
            content = f'''version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

  agent:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      - ollama

volumes:
  ollama_data:
'''
        
        with open(path, "w") as f:
            f.write(content)
    
    @classmethod
    def _generate_readme(cls, path: Path, include_ollama: bool):
        """Generate README with instructions."""
        if include_ollama:
            content = '''# OnsetLab Agent (Docker)

This directory contains everything needed to run your OnsetLab agent in Docker.

## Quick Start

```bash
# Build and run
docker-compose up --build

# Or build manually
docker build -t my-agent .
docker run -p 8000:8000 my-agent
```

## API Usage

```bash
# Health check
curl http://localhost:8000/health

# Chat
curl -X POST http://localhost:8000/chat \\
  -H "Content-Type: application/json" \\
  -d '{"message": "What is 15 + 27?"}'
```

## Interactive Mode

```bash
docker run -it my-agent --interactive
```
'''
        else:
            content = '''# OnsetLab Agent (Docker)

This directory contains everything needed to run your OnsetLab agent in Docker.

**Note:** This setup requires Ollama running separately. Use docker-compose for easy setup.

## Quick Start

```bash
# Start with docker-compose (includes Ollama)
docker-compose up --build

# Wait for Ollama to download the model (first run only)
```

## Manual Setup

If running without docker-compose:

1. Start Ollama: `ollama serve`
2. Pull model: `ollama pull phi3.5`
3. Build: `docker build -t my-agent .`
4. Run: `docker run -p 8000:8000 -e OLLAMA_HOST=http://host.docker.internal:11434 my-agent`

## API Usage

```bash
# Health check
curl http://localhost:8000/health

# Chat
curl -X POST http://localhost:8000/chat \\
  -H "Content-Type: application/json" \\
  -d '{"message": "What is 15 + 27?"}'
```
'''
        
        with open(path, "w") as f:
            f.write(content)
